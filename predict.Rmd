---
title: "Practical Machine Learning- Prediction Assignment"
author: "Pravin Kumar"
output: html_document
---

## Synopsis

The two corresponding datasets of Training and testing have been generated by particpants' accelerometers on the belt, forearm, arm, and dumbell and by  barbell lifted correctly and incorrectly in 5 different ways by six male participants aged between 20-28 years,. Out of "classe" ordinal Variable, Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. In due course, a parsimonious model has been developed from the training dataset to predict on testing dataset for the outcome of 20 different test cases, whether the participants are performing the right exercise "A" or not.

In the process of modelling and prediction, expected out of sample error is taken to be  5%, hence, our prediction model should be with accuracy of atleast 95%. The same has been proven and upgraded with the cross validation of fitted model through "Random Forest" algorithm duly processed on the split up training data set into a dataset of trainingProc and testingProc. Needless to say, our duly developed model is more than 97% accurate and its performance for predicted outcome of 20 different test cases also has been found to be correct.

## Getting and Cleaning Datasets

From the given link: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv, training datset has been downloded from the web and stored in the current working directory from where the csv file has been read through "read.table" function with the argument "na.string" to remove all blank spaces indicating missing values and"#DIV/0!" with "NA", for the ease of  future processing of the data. The training data set has 19622 observations and 160 variables including the outcome variable "classe". Similarly,the testing dataset has been downloaded from the link:https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv and is having only 20 observations and 160 variables includung the variable of "Problem_id" of 20 test cases.

```{r,readingdata,echo=TRUE, eval=TRUE}
training<- read.table("./data/pml-training.csv",header=TRUE,sep= ",",
                      na.strings=c("NA","","#DIV/0!"))
testing<- read.table("./data/pml-testing.csv",header=TRUE,sep= ",",
                     na.strings=c("NA","","#DIV/0!"))
dim(training);dim(testing)
trainingsum<- summary(training);testingsum<- summary(testing)
```

### Cleaning of Datasets 

Before application of most of the modelling algorithm, missing values are observed to be the impediments for the development of model. Hence, either they have to be removed or they are to be imputed by their means of the respective columns. Visualising from the summaries of testing and training datasets, we observe that some of the features are either having only missing values or many of them even have more than 95% of missing values represented by NA. Therefore, even by imputing on the basis of available those less than 5% values will not render a proper model hence, these have been removed from the dataset.

Moreover, further looking into the datasets we observe that the first seven features are  sr.No, Name of participants, raw times and windows which would basically be non contributing to the prediction function, hence, have to be deleted from the datasets.

```{r,cleaningData, echo=TRUE,eval=TRUE}

training <- training[,colSums(is.na(training))/ nrow(training) < 0.95]
testing<- testing[, colSums(is.na(testing))/nrow(testing) < 0.95]
training<- training[,-(1:7)]
testing <- testing[, -(1:7)]
dim(training);dim(testing)
```

Now the datasets of training and testing are having only 53 variables each and observations are same as earlier 19622 and 20 respectively. On close observation the 53rd variable in training dataset is the "classe" having ordinal outcome variable. Similarly,the last 53rd variable in the testing dataset is the "Problem_id" of 20 test cases which are to be predicted after the model fitment. Rest of the features have been compared for their similarity of features indicated in both of the datasets and found to be all same which make testing dataset compatible for prediction through model developed through training dataset.

```{r, comparisonDataset, echo=TRUE,eval=TRUE}
all.equal(colnames(testing[,-53]),colnames(training[,-53]))

```

Now we are ready for the processing of fitting of a model to the training dataset with "classe" as the outcome variable and rest 52 Features as predictors. After finalisation of features in the dataset, we should check for the further generation of covariates or the elimination of any covariate which has zero variability.
```{r,covariates,echo=TRUE,eval=TRUE,message=FALSE,warning=FALSE}

library(caret)
nzerovar<- nearZeroVar(training,saveMetrics=TRUE)
which(nzerovar$nzv == TRUE)

```

As indicated from the integer(0), No feature has any zero variability ie, a constant value. Hence, there is no feature identified to be dropped from the dataset of training.

## Data Processing for Development of Prediction Model 

### Splitting of Dataset Training

The training dataset is having 19622 observations hence is a very large dataset which would take a lot of time and memory space of my computer RAM  which is constrained by 2GB, during the  processing for CART or any other algorithm like randomFroest.Hence, unlike the normal norms of  splitting of training data set into further training and testing datasets for devlopment of model and its cross validation in 60:40, I have decided to split the training dataset into 30:70 training to testing datasets for all future processing.

```{r,splitting, echo=TRUE,eval=TRUE}
set.seed(353)
inTrain<- createDataPartition(training$classe , p= 0.3, list= FALSE)
trainingProc<- training[inTrain,]
testingProc<- training[-inTrain,]
dim(trainingProc);dim(testingProc)

```

### Application of Machine Learning algorithm

I have already decided to have the expected out of Sample error to be 5% and to achieve that we will have to apply such a machine learning algorithm that will give the acuracy of prediction to atleast a level of 95%. Let us apply Classification Tree algorithm first.

### Application of Classification Tree Algorithm

Applying the classification tree algorithm through the train function and rpart method along with preprocessing and cross validation in successive steps for cheking whether the model gives the desired accuracy or not.

```{r,tree,echo=TRUE,eval=TRUE}
library(caret);library(rpart)
modfit_rpart<- train(trainingProc$classe~., method="rpart",data= trainingProc)
# after Preprocessing
modfit_rpart1<- train(trainingProc$classe~., method="rpart",preProcess=c("pca"),
                     data= trainingProc)
# after cross validation even without preProcessing not improving the accuracy:
modfit_rpart2<- train(trainingProc$classe~.,method="rpart",
                      trControl=trainControl(method = "cv"),data= trainingProc)
# combined output of Accuracy of the three stages of fitment of model
acc_rpart<- rbind(
  confusionMatrix(testingProc$classe,predict(modfit_rpart,testingProc))$overall[1],
  confusionMatrix(testingProc$classe,predict(modfit_rpart1,testingProc))$overall[1],
  confusionMatrix(testingProc$classe,predict(modfit_rpart2,testingProc))$overall[1]
)
acc_rpart
```

As shown in the above table it is clear that accuracy of the model in the first stage itself is `r acc_rpart[1,]*100`%. It is to be further noted that even preProcessing with "pca" does not improve the accuracy and rather decreases further to `r acc_rpart[2,]*100`%.This clearly indicates that reduction of colinear covariates is not contributing to the improvement in accuracy of the model. Even after removing the preProcessing, and application of cross validation the accuracy of the fitted model does not improve and almost remained same to `r acc_rpart[3,]*100`%. This is completely disheartening because our target is to acheive the accuracy to the level of 95% whereas our out of sample error from this CART is very high and unacceptable to `r (1-acc_rpart[3,])*100`%. 

### Application of RandomForest Algorithm

As RandomForest algorithm is known for the development of high accuracy model, hence we are going to try this algorithm through train function and  bootstrap sampling, limiting to No of tree formation to 100 due to the constrain of my computer RAM of 2GB. As we have seen that preProcessing does not contribute to the improvement of accuracy in the fitted model, hence, this time we would drop this argument and we would definetly try separately to see upto what extent the cross validation with k= 10 folds will boost up the accuracy of the model.

```{r,randomFolrest,echo=TRUE,eval=TRUE,message=FALSE,warning=FALSE}
library(caret)
library(randomForest)
set.seed(353)
modfit_rf<- train(classe ~.,data= trainingProc,method="rf",prox=TRUE,ntree=100)
# model fitting with cross validation
modfit_rf2<- train(classe ~.,data= trainingProc,method="rf",prox=TRUE,
                   trControl=trainControl(method = "cv", number = 10),ntree=100)
# combined output of accuracy of fitment of model
acc_rf<- rbind(
  confusionMatrix(testingProc$classe,predict(modfit_rf,testingProc))$overall[1],
  confusionMatrix(testingProc$classe,predict(modfit_rf2,testingProc))$overall[1]
  )
acc_rf
modfit_rf2
confusionMatrix(testingProc$classe,predict(modfit_rf2,testingProc))

```

As expected, the accuracy achieved through the method randomForest is quite high and is `r acc_rf[1,]*100`%. yet we find that despite the use of cross validation there is no improvement in accuracy and rather it has decreased marginally to `r acc_rf[2,]*100`%. Maximum accuracy has been used to pick up the optimal model at the  tuning parameter "mtry" = 27.

### Out of Sample Error

Out of sample error rate is the expected error in our prediction through the fitted model on any testing data and = (1- Accuracy). Hence, estimated error rate with cross validation is equal to `r (1-acc_rf[2,])*100`%, which is much lower to that of our target of 5%, hence, the fitted model is acceptable.

## Conclusion and Prediction with Testing Dataset

RandomForest method along with train function has yielded a parsimonious model with a good accuracy level to `r acc_rf[2,]*100`% to predict any testing dataset.The prediction outcome on testing dataset has also been found to be true.

```{r,testingDataset,echo=TRUE,eval=TRUE}
pred<- predict(modfit_rf2,newdata=testing)
# generation of files along with prediction
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(pred)
# prediction for testing dataset
pred

```

